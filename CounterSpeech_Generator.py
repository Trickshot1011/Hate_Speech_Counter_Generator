# -*- coding: utf-8 -*-
"""Final_Big_Data_Project_Sid_added_rouge_large.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VQFfdTyMURBmPlGnHKfcSadkFhnIopkn
"""

!nvidia-smi

!pip install clean-text --quiet
!pip install transformers --quiet
!pip install torch --quiet
!pip install pytorch-lightning --quiet
!pip install tokenizers --quiet
!pip install sentencepiece --quiet

import argparse
import glob
import os
import json
import time
import logging
import random
import re
from itertools import chain
from string import punctuation

import pandas as pd
import numpy as np
import torch
import pathlib as Path
from torch.utils.data import Dataset, DataLoader
from cleantext import clean
print(torch.__version__)

# !pip install git+https://github.com/PyTorchLightning/pytorch-lightning
import pytorch_lightning as pl
print(pl.__version__)
from sklearn.model_selection import train_test_split



from transformers import (

    AdamW,
    T5ForConditionalGeneration,
    T5Tokenizer,
    get_linear_schedule_with_warmup


)
from tqdm.auto import tqdm
print(torch.__version__)


!pip install termcolor -q
import textwrap
from termcolor import colored

pl.seed_everything(10)

"""## Dataset"""

df = pd.read_csv('https://raw.githubusercontent.com/marcoguerini/CONAN/master/Multitarget-CONAN/Multitarget-CONAN.csv',encoding='latin-1')
df.head()

df = df[['HATE_SPEECH','COUNTER_NARRATIVE','TARGET']]
df.columns = ['hate','counter','category']

df.head(10)

"""# Data Pre-processing"""

df.shape

# cleaning the data of all emoji's and lowering them as well.
from bs4 import BeautifulSoup
def pre_processing(data_points):
    new_data_points = data_points.lower()
    new_data_points_cleaned = clean(new_data_points, no_emoji=True)

    pattern = r"http\S+"

    new_data_point_cleaned_without_html_links = re.sub(pattern,'', new_data_points_cleaned, flags=re.MULTILINE)
    return new_data_point_cleaned_without_html_links

df['hate'] = df['hate'].apply(pre_processing)
df['counter'] = df['counter'].apply(pre_processing)

df.head()

def average_characters(data_frame):

    hateSpeech_len, counterSpeech_len = 0, 0
    hateSpeech_words_distribution = []
    counterSpeech_words_distribution = []

    max_number_of_hate_speech_words = -1
    min_number_of_hate_speech_words = 1000

    max_number_of_counter_speech_words = -1
    min_number_of_counter_speech_words = 1000


    for indx in range(data_frame.shape[0]):

        # collecting, total number of words in hate speech and words in each index

        number_of_words_hate = len(data_frame.iloc[indx]['hate'].split())
        hateSpeech_len += number_of_words_hate
        hateSpeech_words_distribution.append(number_of_words_hate)
        max_number_of_hate_speech_words = max(max_number_of_hate_speech_words, number_of_words_hate)
        min_number_of_hate_speech_words = min(min_number_of_hate_speech_words, number_of_words_hate)

        # collection total number of words in counter speech list and also the words in each index

        number_of_words_counter = len(data_frame.iloc[indx]['counter'].split())
        counterSpeech_len += number_of_words_counter
        counterSpeech_words_distribution.append(number_of_words_counter)
        max_number_of_counter_speech_words = max(max_number_of_counter_speech_words, number_of_words_counter)
        min_number_of_counter_speech_words = min(min_number_of_counter_speech_words, number_of_words_counter)


    hate_speech_avg_len = hateSpeech_len//data_frame.shape[0]
    counter_speech_avg_len = counterSpeech_len//data_frame.shape[0]

    return [hate_speech_avg_len, max_number_of_hate_speech_words, min_number_of_hate_speech_words], [counter_speech_avg_len, max_number_of_counter_speech_words, min_number_of_counter_speech_words], hateSpeech_words_distribution, counterSpeech_words_distribution

hs_data, cs_data, hs_distribution, cs_distribution = average_characters(df)

#hate speech data
print("Hate Speech data quantification")
print('****************************')
print("Average Hate Speech Len: ", hs_data[0])
print("Maximum Hate Speech Len: ", hs_data[1])
print("Minimum Hate Speech Len: ", hs_data[2])
print('****************************')
print('\n\n')
print("Counter Speech data quantification")
print('****************************')
print("Average Hate Speech Len: ", cs_data[0])
print("Maximum Hate Speech Len: ", cs_data[1])
print("Minimum Hate Speech Len: ", cs_data[2])
print('****************************')

# hate speech data distribution
import matplotlib.pyplot as plt

data = hs_distribution

plt.hist(data, bins=15)
plt.grid()
plt.show()

import matplotlib.pyplot as plt

data = cs_distribution

plt.hist(data, bins=15)
plt.grid()

plt.show()

"""## Tokenization"""

MODEL_NAME = 't5-large'

tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)

"""### Trying out the tokenizer"""

example_token = tokenizer(
    'Testing this tokenizer is difficult',
    'Water is blue, or is it?'
)

example_token.keys()

example_token['input_ids']

example_token['attention_mask']

pred_inputs = [
    tokenizer.decode(input_id, skip_special_tokens = True, clean_up_tokenization_spaces=True)
    for input_id in example_token['input_ids']
]

' '.join(pred_inputs)

"""### Trying Tokenizer of smaple data point"""

data_point = df.iloc[50]

data_point['hate']

data_point['counter']



encoding = tokenizer(
    data_point['hate'],
    data_point['category'],
    max_length=25,
    padding='max_length',
    truncation='only_first', # to make sure that category is not truncated
    return_attention_mask=True,
    add_special_tokens=True,
    return_tensors='pt'
)

print(encoding['input_ids'])
tokenizer.decode(encoding['input_ids'].squeeze())

tokenizer.decode(encoding['input_ids'].squeeze())

answer_encoding = tokenizer(
    data_point['counter'],
    max_length=40,
    padding='max_length',
    truncation=True,
    return_attention_mask=True,
    add_special_tokens=True,
    return_tensors='pt'
)

tokenizer.decode(answer_encoding['input_ids'].squeeze())

"""## Creating Hugging face Dataset"""

class Conan_Hugging_Dataset(Dataset):

    def __init__(
        self,
        data: pd.DataFrame,
        tokenizer: T5Tokenizer,
        source_max_token_len: int = 25,
        target_max_token_len: int = 40,
    ):

        self.tokenizer = tokenizer
        self.data = data
        self.source_max_token_len = source_max_token_len
        self.target_max_token_len = target_max_token_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self,index: int):
        data_points = self.data.iloc[index]

        source_encoding = tokenizer(
            data_points['hate'],
            data_points['category'],
            max_length=self.source_max_token_len,
            padding='max_length',
            truncation='only_first',
            return_attention_mask=True,
            add_special_tokens=True,
            return_tensors='pt'
        )

        target_encoding = tokenizer(
            data_points['counter'],
            max_length=self.target_max_token_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            add_special_tokens=True,
            return_tensors='pt'
        )

        labels = target_encoding['input_ids']
        labels[labels == 0] = -100

        return dict(
            hate = data_points['hate'],
            category = data_points['category'],
            counter = data_points['counter'],
            input_ids = source_encoding['input_ids'].flatten(),
            attention_mask = source_encoding['attention_mask'].flatten(),
            labels = labels.flatten()
        )

dataset_check = Conan_Hugging_Dataset(df, tokenizer)

count = 0
for rows in dataset_check:

    print(rows['hate'])
    print(rows['counter'])
    print(rows['input_ids'][:10])
    print(rows['labels'][:10])
    print('-------------------------------------------------')
    if count > 2:
        break

    count += 1

seen_data = df[:4000]
unseen_data = df[4000:]

model_train_data, model_test_data = train_test_split(seen_data, test_size=0.1)

print("Shapes of split")
print("Train Data size: ",model_train_data.shape)
print("Test Data size: ",model_test_data.shape)

"""## Defining the setup for data loader(each for train, test and val)"""

class CounterSpeechGeneratorModule(pl.LightningDataModule):

    def __init__(
        self,
        model_train_data: pd.DataFrame,
        test_df: pd.DataFrame,
        tokenizer: T5Tokenizer,
        batch_size: int = 3,
        source_max_token_len: int = 25,
        target_max_token_len: int = 40,
    ):

        super().__init__()
        self.batch_size = batch_size
        self.model_train_data = model_train_data
        self.test_df = test_df
        self.tokenizer = tokenizer
        self.source_max_token_len = source_max_token_len
        self.target_max_token_len = target_max_token_len


    def setup(self, stage=None):

        self.train_dataset = Conan_Hugging_Dataset(
            self.model_train_data,
            self.tokenizer,
            self.source_max_token_len,
            self.target_max_token_len
        )

        self.test_dataset = Conan_Hugging_Dataset(
            self.test_df,
            self.tokenizer,
            self.source_max_token_len,
            self.target_max_token_len
        )

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size = self.batch_size,
            shuffle = True,
            num_workers = 10
        )

    def val_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size = 1,
            num_workers = 10
        )

    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size = 1,
            num_workers = 10
        )

BATCH_SIZE = 15
N_EPOCHS = 6

data_module = CounterSpeechGeneratorModule(model_train_data, model_test_data, tokenizer, batch_size = BATCH_SIZE)
data_module.setup()

model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)

model.config

"""## Modeling"""

class CounterSpeechGeneratorModel(pl.LightningModule):

    def __init__(self):

        super().__init__()
        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)

    def forward(self, input_ids, attention_mask, labels=None):

        output = self.model(
            input_ids = input_ids,
            attention_mask = attention_mask,
            labels = labels
        )
        return output.loss, output.logits

    def training_step(self, batch, batch_idx):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']
        loss, outputs = self(input_ids, attention_mask, labels)
        self.log('train_loss',loss,prog_bar=True,logger=True)
        return loss

    def validation_step(self, batch, batch_idx):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']
        loss, outputs = self(input_ids,attention_mask, labels)
        self.log('val_loss',loss,prog_bar=True,logger=True)
        return loss

    def test_step(self, batch, batch_idx):
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']
        loss, outputs = self(input_ids, attention_mask, labels)
        self.log('test_loss',loss,prog_bar=True,logger=True)
        return loss

    def configure_optimizers(self):
        return AdamW(self.parameters(), lr = 0.0001)

model = CounterSpeechGeneratorModel()

from pytorch_lightning.callbacks import ModelCheckpoint
checkpoint_callbacks = ModelCheckpoint(
    dirpath="checkpoints",
    filename="best-checkpoint",
    save_top_k=1,
    verbose=True,
    monitor="val_loss",
    mode="min"
)

from pytorch_lightning.loggers import TensorBoardLogger
logger = TensorBoardLogger('lightning_logs',name = 'CounterSpeechGeneratorModel-Eval')
trainer = pl.Trainer(
    logger = logger,
    callbacks = [checkpoint_callbacks],
    max_epochs=N_EPOCHS,
    devices = 1,
    accelerator="gpu",
    enable_progress_bar=True
)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir ./lightning_logs

trainer.fit(model, data_module)

best_model = CounterSpeechGeneratorModel.load_from_checkpoint('/content/checkpoints/best-checkpoint.ckpt')
best_model.freeze()

unseen_sample = unseen_data.iloc[50]

unseen_sample['hate']

unseen_sample['counter']

def generate_counter_speech(point=unseen_sample):
  source_encoding = tokenizer(
      point['hate'],
      point['category'],
      max_length=25,
      padding='max_length',
      truncation='only_first',
      return_attention_mask=True,
      add_special_tokens=True,
      return_tensors='pt'
  )

  source_encoding.to('cuda')

  generated_ids = best_model.model.generate(
      input_ids = source_encoding['input_ids'],
      attention_mask=source_encoding['attention_mask'],
      max_length=40,
      repetition_penalty=2.5,
      length_penalty=1.0,
      early_stopping=True,
      use_cache=True
  )
  preds = [
      tokenizer.decode(generate_id,skip_special_tokens=True,clean_up_tokenization_spaces=True)
      for generate_id in generated_ids
  ]
  return "".join(preds)

output = generate_counter_speech(unseen_sample)
print(output)

dictionary_for_dataframe = {}

list_of_given_counter_speech = []
list_of_generated_counter_speech = []
count = 0
for indx in range(len(unseen_data)):
  count += 1

  data_point = unseen_data.iloc[indx]
  list_of_given_counter_speech.append(data_point['counter'])
  output = generate_counter_speech(data_point)
  list_of_generated_counter_speech.append(output)


dictionary_for_dataframe['given_counter'] = list_of_given_counter_speech
dictionary_for_dataframe['generated_counter'] = list_of_generated_counter_speech

# creating dataframe

counter_speech_dataframe = pd.DataFrame(dictionary_for_dataframe)
counter_speech_dataframe.head(10)

counter_speech_dataframe.to_csv('counter_speech_generate_vs_given_large.csv',index=False)

load_dataset = pd.read_csv('counter_speech_generate_vs_given_large.csv')

load_dataset.head(10)

import locale
locale.getpreferredencoding = lambda: "UTF-8"

! pip install datasets transformers torch evaluate nltk rouge_score

from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
#import evaluate

"""# Results Rouge score"""

#read csv
import pandas as pd
df=pd.read_csv("/content/counter_speech_generate_vs_given_large.csv")

print(df)

list_act=df.iloc[:,0]
print(list(list_act))

list_pred=df.iloc[:,1]
print(list(list_pred))

!pip uninstall evaluate
!pip install evaluate

# import evaluate
import evaluate('utf-8')
rouge = evaluate.load('rouge')


results = rouge.compute(predictions=list_pred,references=list_act,use_aggregator=True)


print(results)